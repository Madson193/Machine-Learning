# -*- coding: utf-8 -*-
"""hw2_311540001

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10sh6I_174RSX78ZIFDrwwqUF92AZIek0
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os

# you can choose one of the following package for image reading/processing
import cv2
from google.colab.patches import cv2_imshow
import PIL

"""## This part is to include all the functions"""

def one_hot(t):
  temp = np.zeros((10,1))
  temp[t] = 1
  return temp

def shuffle_and_split(data,test_percent = 0.2,axis=1):
  df=pd.DataFrame.from_records(data)
  df = df.sample(frac=1)
  # print(df)
  temp = np.array(df)
  temp = np.transpose(temp)
  shape = temp.shape
  # print(shape)
  test_data = temp[0:shape[0]-10,0:round(shape[1]*test_percent)]
  test_target = temp[shape[0]-10:shape[0],0:round(shape[1]*test_percent)]
  # print(test_target)

  train_data = temp[0:shape[0]-10,round(shape[1]*test_percent):shape[1]]
  train_target = temp[shape[0]-10:shape[0], round(shape[1]*test_percent):shape[1]]

  return train_data.transpose(), train_target.transpose(), test_data.transpose(), test_target.transpose()


#------------------------------------------------------------------------------
##/ 1.2 Logistic Regression
def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum()
    # return 1/(1 + np.exp(-x))

def error_calculation(y,t):
  error = 0

  N = y.shape[0]
  t_ = np.argmax(t,axis=1)
  y_ = np.argmax(y,axis=1)
  # print(y)
  for n in range(y.shape[0]):
      error = error - (-y[n][y_[n]]+t[n][t_[n]])*1/N*np.log(y[n][t_[n]]+0.00001) ## Note about the error mistake
  return error

def accuration_calulation(X_,W,t):
  y_ = np.dot(X_,W)
  
  y_ = np.argmax(y_,axis=1)
  t_ = np.argmax(t,axis=1)
  correct = 0
  for i in range (X_.shape[0]):
    if y_[i]==t_[i]:
      correct += 1
      # print("nice")
  # print('correct = ', correct)
  accuracy = correct*100/X_.shape[0]
  return accuracy

def diagonal(R):
  N = R.shape[0]
  diag_matrix = np.zeros((N,N))
  for i in range(N):
    diag_matrix[i][i] = R[i][i]
  
  return diag_matrix

"""# Data normalization 
We can use the below equation to normalize the input data.

![Data-normalization.png](data:image/png;base64,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)
"""

# path = "/content/drive/MyDrive/Colab Notebooks/Homework2/MNIST/{}/{}.jpg"
# temp = os.listdir(path) 
# number_files = len(temp)
number_files = 128
# print(number_files)

path = r"/content/drive/MyDrive//Colab Notebooks/Homework2/MNIST/0/0.jpg"

# Normalize the data
img = cv2.imread(path)
img = np.array(img)
img = (img-img.max())/(img.max() - img.min())
print(img.shape)
img = img.flatten()
img = img.reshape(-1,1)
t_vector = one_hot(0)
print(t_vector)
temp = np.concatenate((img,t_vector),axis=0)
print(temp.shape)




# cv2_imshow(img)
large_matrix = []
class_num = 10
for k in range(class_num):
  path = r"/content/drive/MyDrive//Colab Notebooks/Homework2/MNIST/{}/{}.jpg"
  t_vector = one_hot(k)
  for i in range(number_files):
    # print(temp_path)
    temp_path = path.format(k,i)
    # print(temp_path)
    img = cv2.imread(temp_path)
    img = np.array(img)
    img = (img-img.max())/(img.max() - img.min())
    img = img.flatten()
    # print(k)
    img = img.reshape(-1,1)
    
    # print(t_vector)
    temp = np.concatenate((img,t_vector),axis=0)
    large_matrix.append(temp) 
    ##/ dim(large_matrix) = (128*10,2362)
    ##/ large_matrix[k][2362-10:2362] is the target

large_matrix = np.array(large_matrix)
# print (large_matrix[4][2362-10:2362])
  # print(temp.shape)

"""# **1.1 Least squares for classification**
## Finding the weight 
"""

temp_matrix = (large_matrix)
shape = temp_matrix.shape
# print(shape)
temp_matrix = temp_matrix.reshape((shape[0],shape[1]))
# temp_matrix = np.transpose(temp_matrix) # ((2362, 1280)) 2362 is flatten + one_hot, 1280 is N sample

train_data, train_target, test_data, test_target = shuffle_and_split(temp_matrix,test_percent=0.2)
train_target.shape
a = np.dot(train_data.transpose(),train_data)
a = np.linalg.pinv(a)
b = np.dot(train_data.transpose(),train_target)
W = np.dot(a,b)

"""## Calculate the prediction for training and testing data using the trained Weight"""

##/ Test for traning data
y = np.dot(train_data,W)

i = np.argmax(y,axis=1)
i_target = np.argmax(train_target,axis=1)
##/ need to check for the error value
train_error = error_calculation(y,train_target)
 
print(train_error)
correct = 0
incorrect = 0
for k in range(i.shape[0]):
  if i[k]== i_target[k]:
    correct += 1
  else:
    incorrect +=1
    # print(i[k], " ", i_target[k])

  
print ("the accuracy of training data = ", (correct/i.shape[0])*100)

plt.bar("Correct prediction",(correct/i.shape[0])*100,label="correct prediction")
plt.bar("Incorrect prediction",(incorrect/i.shape[0])*100,label = "incorrect prediction")
plt.ylabel('Percentage %')
plt.title("Prediction correct's percentage (training data)")
plt.legend()
plt.show()

plt.plot(i,marker="o",linestyle="None")
plt.plot(i_target,marker="x",linestyle="None")
plt.title("The result for training data")
plt.ylabel("Classes")
plt.xlabel("Samples")
plt.show()


##/ Test for target data
y = np.dot(test_data,W)

i = np.argmax(y,axis=1)
i_target = np.argmax(test_target,axis=1)
test_error = error_calculation(y,test_target)
 
print(test_error)
 

correct1 = 0
incorrect1 = 0
for k in range(i.shape[0]):
  if i[k]== i_target[k]:
    correct1 += 1
  else:
    incorrect1 += 1
    # print(i[k], " ", i_target[k])

print ("the accuracy of testing data = ", (correct1/i.shape[0])*100)
plt.bar("Correct prediction",(correct1/i.shape[0])*100,label="correct prediction")
plt.bar("Incorrect prediction",(incorrect1/i.shape[0])*100,label = "incorrect prediction")
plt.ylabel('Percentage %')
plt.title("Prediction correct's percentage (testing data)")
plt.legend()
plt.show()

plt.plot(i,marker="o",linestyle="None")
plt.plot(i_target,marker="x",linestyle="None")
plt.title("The result for testing data")
plt.ylabel("Classes")
plt.xlabel("Samples")
plt.show()

"""# **1.2 Logistic regression**
## **Method1**: Batch gradient descent
"""

# general setting
epoch = 100
k = 10
temp_matrix = (large_matrix)
shape = temp_matrix.shape
temp_matrix = temp_matrix.reshape((shape[0],shape[1]))
train_data, train_target, test_data, test_target = shuffle_and_split(temp_matrix,test_percent=0.2)


# Batch gradient descent

W_new =  np.zeros((train_data.shape[1],k)) # 2352*10
X_ = train_data
t = train_target
# print(t[1,:])

X_test = test_data
t_test = test_target

# y = softmax(np.dot(X_,W_new))
# print(y)
# y_test = softmax(np.dot(X_test,W_new))

# R = np.dot(y,np.transpose(y))
# z = np.dot(X_,W_new)-np.dot(np.linalg.pinv(R),y-t)
# a = np.dot(np.transpose(X_),R)
# XRX = np.dot(a,X_)
# b = np.dot(a,z)
# W_new = np.dot(np.linalg.pinv(XRX),b)

train_accuracy = []
test_accuracy = []

train_error = []
test_error = []

for i in range(10):
  # y = softmax(np.dot(X_,W_new))
  # a = (np.argmax(y,axis=1)) 
  # print(a.shape)
  # error_train = error_calculation(y,t)
  # accuration_train = accuration_calulation(X_,W_new,t)
  # y_test = softmax(np.dot(X_test,W_new))
  # error_test = error_calculation(y_test,t_test)
  # accuration_test = accuration_calulation(X_test,W_new,t_test)

  # R = np.diag(np.ravel(y*(1-y)))
  # z1 =(np.dot(X_,W_new))
  # z2 = np.dot(np.linalg.pinv(R),y-t)
  # z = z1-z2
  # a = np.dot(np.transpose(X_),R)
  # XRX_1 = np.linalg.pinv(np.dot(a,X_))
  # b = np.dot(a,z)
  # W_new = np.dot(XRX_1,b)

  y = softmax(np.dot(X_,W_new))
  error_train = error_calculation(y,t)
  accuration_train = accuration_calulation(X_,W_new,t)
  train_accuracy.append(accuration_train)
  train_error.append(error_train)

  y_test = softmax(np.dot(X_test,W_new))
  error_test = error_calculation(y_test,t_test)
  accuration_test = accuration_calulation(X_test,W_new,t_test)
  test_accuracy.append(accuration_test)
  test_error.append(error_test)

  

  R = np.dot(y,np.transpose(1-y)) #Keep in mind that R is a diagonal matrix
  R = diagonal(R)
  a = np.dot(np.transpose(X_),R)
  # H = np.dot(a,X_)
  # H = H/X_.shape[0]
  grad = np.dot(X_.transpose(),y-t)
  # print(error_train,error_test)
  # print("epoch: ", i)
  # print(accuration_train, accuration_test)
  # W_new = W_new - np.dot(np.linalg.pinv(H),grad)
  W_new = W_new - 0.001*grad

plt.plot(train_accuracy,label='Train accuracy')
plt.plot(test_accuracy,label="Test accuracy")
plt.title("a) Batch gradient descent Accuracy")
plt.legend()
plt.show()

plt.plot(train_error,label="Train error")
plt.plot(test_error,label="Test error")
plt.title("a) Batch gradient descent Error")
plt.legend()
plt.show()

labels = ['Train accuracy', 'Test accuracy']
values = [train_accuracy[9], test_accuracy[9]]


fig, ax = plt.subplots()

ax.barh(labels, values)
for i, value in enumerate(values):
    ax.text(value + 3, i, str(value))
xmin, xmax = ax.get_xlim()
ax.set_xlim(xmin, 1.1*xmax)

plt.title("Accuracy of Batch gradient descent method")
plt.show()

labels = ['Train error', 'Test error']
values = [train_error[9], test_error[9]]


fig, ax = plt.subplots()

ax.barh(labels, values)
for i, value in enumerate(values):
    ax.text(value + 3, i, str(value))
xmin, xmax = ax.get_xlim()
ax.set_xlim(xmin, 1.1*xmax)
plt.title("Error of Batch gradient descent method")
plt.show()

"""#

## **Method 2**: Stochastic gradient descent
"""

# general setting
epoch = 100
k = 10
temp_matrix = (large_matrix)
shape = temp_matrix.shape
temp_matrix = temp_matrix.reshape((shape[0],shape[1]))
train_data, train_target, test_data, test_target = shuffle_and_split(temp_matrix,test_percent=0.2)


# Batch gradient descent

W_new =  np.zeros((train_data.shape[1],k)) # 2352*10
X_train = train_data
t_train = train_target
# print(t[1,:])

X_test = test_data
t_test = test_target


accuracy = []

train_accuracy = []
test_accuracy = []

train_error = []
test_error = []
for i in range (10):
  print ("epoch = ",i)
  y = softmax(np.dot(X_train,W_new))
  error_train = error_calculation(y,t_train)
  accuration_train = accuration_calulation(X_train,W_new,t_train)
  train_accuracy.append(accuration_train)
  train_error.append(error_train)

  y_test = softmax(np.dot(X_test,W_new))
  error_test = error_calculation(y_test,t_test)
  accuration_test = accuration_calulation(X_test,W_new,t_test)
  test_accuracy.append(accuration_test)
  test_error.append(error_test)
  for k in range(X_train.shape[0]):
    X_ = X_train[k,:]
    X_ = np.reshape(X_,(1,X_train.shape[1]))
    t = t_train[k,:]
    y = softmax(np.dot(X_,W_new))
    if (np.argmax(y) == np.argmax(t)):
      correct += 1
      # print(correct)
    # error_train = error_calculation(y,t)
    # accuration_train = accuration_calulation(X_,W_new,t)
    y_test = softmax(np.dot(X_test,W_new))
    # error_test = error_calculation(y_test,t_test)
    # accuration_test = accuration_calulation(X_test,W_new,t_test)
    R = np.dot(y,np.transpose(1-y)) #Keep in mind that R is a diagonal matrix
    R = diagonal(R)
    a = np.dot(np.transpose(X_),R)
    H = np.dot(a,X_)
    grad = np.dot(X_.transpose(),y-t)
    # print(error_train,error_test)
    # print(accuration_train, accuration_test)
    W_new = W_new - 0.001*grad

plt.plot(train_accuracy,label='Train accuracy')
plt.plot(test_accuracy,label="Test accuracy")
plt.title("Batch gradient descent Accuracy")
plt.ylabel("Percentage %")
plt.xlabel("Epoch")
plt.legend()
plt.show()

plt.plot(train_error,label="Train error")
plt.plot(test_error,label="Test error")
plt.title("Batch gradient descent Error")
plt.ylabel("Error")
plt.xlabel("Epoch")
plt.legend()
plt.show()

labels = ['Train accuracy', 'Test accuracy']
values = [train_accuracy[9], test_accuracy[9]]


fig, ax = plt.subplots()

ax.barh(labels, values)
for i, value in enumerate(values):
    ax.text(value + 3, i, str(value))
xmin, xmax = ax.get_xlim()
ax.set_xlim(xmin, 1.1*xmax)

plt.title("Accuracy of Stochastic gradient descent method")
plt.show()

labels = ['Train error', 'Test error']
values = [train_error[9], test_error[9]]


fig, ax = plt.subplots()

ax.barh(labels, values)
for i, value in enumerate(values):
    ax.text(value + 3, i, str(value))
xmin, xmax = ax.get_xlim()
ax.set_xlim(xmin, 1.1*xmax)
plt.title("Error of Stochastic gradient descent method")
plt.show()

"""## **Method 3**: Mini-batch SGD"""

temp_matrix = (large_matrix)
shape = temp_matrix.shape
temp_matrix = temp_matrix.reshape((shape[0],shape[1]))
train_data, train_target, test_data, test_target = shuffle_and_split(temp_matrix,test_percent=0.2)

k = 10
batch_num = 5
sample_per_batch = np.int(np.round(train_data.shape[0]/batch_num)) #256


W_new =  np.zeros((train_data.shape[1],k)) # 2352*10
X_train = train_data
t_train = train_target
# print(t[1,:])

X_test = test_data
t_test = test_target



accuracy = []

average_train_error = []
average_test_error = []
average_train_accuracy = []
average_test_accuracy = []




for i in range (10):
  print ("epoch = ",i)

  train_accuracy = []
  test_accuracy = []

  train_error = []
  test_error = []

  for n in range(batch_num):
    train_temp = X_train[n*sample_per_batch:(n+1)*sample_per_batch,:]
    target_temp = t_train [n*sample_per_batch:(n+1)*sample_per_batch,:]
    
    y = softmax(np.dot(train_temp,W_new))
    error_train = error_calculation(y,target_temp)
    accuration_train = accuration_calulation(train_temp,W_new,target_temp)
    train_accuracy.append(accuration_train)
    train_error.append(error_train)

    y_test = softmax(np.dot(X_test,W_new))
    error_test = error_calculation(y_test,t_test)
    accuration_test = accuration_calulation(X_test,W_new,t_test)
    test_accuracy.append(accuration_test)
    test_error.append(error_test)

    
    for k in range(train_temp.shape[0]):
      X_ = train_temp[k,:]
      X_ = np.reshape(X_,(1,train_temp.shape[1]))
      t = target_temp[k,:]
      y = softmax(np.dot(X_,W_new))
      if (np.argmax(y) == np.argmax(t)):
        correct += 1
        # print(correct)
      # error_train = error_calculation(y,t)
      # accuration_train = accuration_calulation(X_,W_new,t)
      y_test = softmax(np.dot(X_test,W_new))
      # error_test = error_calculation(y_test,t_test)
      # accuration_test = accuration_calulation(X_test,W_new,t_test)
      R = np.dot(y,np.transpose(1-y)) #Keep in mind that R is a diagonal matrix
      R = diagonal(R)
      a = np.dot(np.transpose(X_),R)
      H = np.dot(a,X_)
      grad = np.dot(X_.transpose(),y-t)
      # print(error_train,error_test)
      # print(accuration_train, accuration_test)
      W_new = W_new - 0.001*grad

  average_train_accuracy.append(np.average(train_accuracy))
  average_test_accuracy.append(np.average(test_accuracy))
  average_train_error.append(np.average(train_error))
  average_test_error.append(np.average(test_error))

plt.plot(average_train_accuracy,label='Train accuracy')
plt.plot(average_test_accuracy,label="Test accuracy")
plt.title("Batch gradient descent Accuracy")
plt.ylabel("Percentage %")
plt.xlabel("Epoch")
plt.legend()
plt.show()

plt.plot(average_train_error,label="Train error")
plt.plot(average_test_error,label="Test error")
plt.title("Batch gradient descent Error")
plt.ylabel("Error")
plt.xlabel("Epoch")
plt.legend()
plt.show()

labels = ['Train accuracy', 'Test accuracy']
values = [average_train_accuracy[9], average_test_accuracy[9]]


fig, ax = plt.subplots()

ax.barh(labels, values)
for i, value in enumerate(values):
    ax.text(value + 3, i, str(value))
xmin, xmax = ax.get_xlim()
ax.set_xlim(xmin, 1.1*xmax)

plt.title("Accuracy of Mini-batch SGD method")
plt.show()

labels = ['Train error', 'Test error']
values = [average_train_error[9], average_test_error[9]]


fig, ax = plt.subplots()

ax.barh(labels, values)
for i, value in enumerate(values):
    ax.text(value + 3, i, str(value))
xmin, xmax = ax.get_xlim()
ax.set_xlim(xmin, 1.1*xmax)
plt.title("Error of Mini-batch SGD method")
plt.show()

"""## **1.2 c**
As we can observe from the result, the Mini-batch SGD gives the best result, with accuracy of training and testing is 94 and 84 percent. The second best performance is the SGD method with accuracy 93, 85 percent for training and testing data. The worst method is batch GD,  with 44 and 34 percent for accuracy. From these method, we learn that it is more efficiency to divide the data into multiple small batch. One more thing, from the Mini-batch SGD and SGD method, we see that although one method gives better training accuracy, it doesn't mean it will have a better testing accuracy.

## **1.3**
In 1.1 we can directly calculate the Weight, then apply it to predict the output without updating the weight. In problem 1.2 we use different approach to minimize the error, that is using gradient descent, which means we will have to update the weight after iterating each batch.For the second approach, we will let the weight to have a chance to loop through the data multiple times so that it can minimize the error and increase the accuracy.

---


# **2. Gaussian Process for Regression**

## Function for problem 2
"""

def data2matrix(x):
  # concate data already passed through basis function
  a1 = np.ones((x.shape[0],1))
  a2 = x 
  matrix = np.concatenate((a1,a2),axis=1)
  M = 2
  for i in range (2,M+1):
    a = np.power(x,i)
    matrix = np.concatenate((matrix,a),axis=1)

  
  return matrix 

def diagonal(x):
  N = x.shape[0]
  diag = np.zeros((N,1))
  for i in range(N):
    diag[i] = x[i][i]
  return diag
    

def kernel(X1,X2):
  result = np.dot(X1,np.transpose(X2))
  return result

# def kernel(X1,X2):
#   result = np.exp(-np.linalg.norm(X1-X2)/2)
#   return result
# def kernel(x1, x2, l=1.0, sigma_f=1.0):
#     """More efficient approach."""
#     dist_matrix = np.sum(x1**2, 1).reshape(-1, 1) + np.sum(x2**2, 1) - 2 * np.dot(x1, x2.T)
#     return  np.exp(-0.5 / l ** 2 * dist_matrix)

def data_split(x,t,test_percent = 0.5):
  data = np.concatenate((x,t),axis=1)
  df = pd.DataFrame.from_records(data)
  df = df.sample(frac=1)

  data = np.array(df)
  test = data[0:round(data.shape[0]*test_percent), :]
  train = data[round(data.shape[0]*test_percent)-1:data.shape[0],:]

  train_data = train[:,0].reshape((train.shape[0],1))
  train_target = train[:,1].reshape((train.shape[0],1))
  test_data = test[:,0].reshape((test.shape[0],1))
  test_target = test[:,1].reshape((test.shape[0],1))

  # print(train_data.shape)
  X_train = data2matrix(train_data)
  # print(train_data[0,:])
  # print(X_train[0,:])
  
  T_train = train_target
  X_test = data2matrix(test_data)
  T_test = test_target

  return test_data, test_target,X_train, T_train, X_test, T_test

def error_cal(x,t):
  error = 0 
  N = x.shape[0]
  for i in range(x.shape[0]):
    error = error + 1/N*(x[i]-t[i])**2

  error = np.sqrt(error)
  return error

# def cal_C(X_train,X_test,k):
#   C = 


#-------------------------------------------------------------------------
# for second part 
def data_split_gaussian(x,t,test_percent = 0.5):
  data = np.concatenate((x,t),axis=1)
  df = pd.DataFrame.from_records(data)
  df = df.sample(frac=1)

  data = np.array(df)
  test = data[0:round(data.shape[0]*test_percent), :]
  train = data[round(data.shape[0]*test_percent)-1:data.shape[0],:]

  train_data = train[:,0].reshape((train.shape[0],1))
  train_target = train[:,1].reshape((train.shape[0],1))
  test_data = test[:,0].reshape((test.shape[0],1))
  test_target = test[:,1].reshape((test.shape[0],1))

  # print(train_data.shape)
  X_train = train_data
  # print(train_data[0,:])
  # print(X_train[0,:])
  
  T_train = train_target
  X_test = test_data
  T_test = test_target

  return test_data, test_target,X_train, T_train, X_test, T_test
def exp_kernel (X1,X2,type):

  if type == "linear":
    the1, the2,the3, the4 = 0, 0, 0, 1
  elif type == "squared":
    the1, the2,the3, the4 = 1, 1, 0, 0
    # dist_matrix = np.sum(x1**2, 1).reshape(-1, 1) + np.sum(x2**2, 1) - 2 * np.dot(x1, x2.T)
    # result = sigma_f ** 2 * np.exp(-0.5 / l ** 2 * dist_matrix)
  elif type == "exp1":
    the1, the2,the3, the4 = 1, 1, 0, 16
  elif type == "exp2":
    # print("exp2")
    the1, the2,the3, the4 = 1, 1, 16,0
  # print(the1)
  
  matrix = []
  for k in range (X2.shape[0]):
    result = []
    for i in range (X1.shape[0]):
      temp = the1*np.exp(-the2/2*np.dot((X1[i]-X2[k]).transpose(),X1[i]-X2[k])) + the3 + the4*np.dot(X1[i],X2[k].transpose())
      result.append(temp)
      result1 = np.array(result)
    matrix.append(result)

  matrix = np.array(matrix)
  # print(matrix.shape)
  return matrix.transpose()

def mean_cov (X_train,T_train,test):
  
  X_test = data2matrix(test)
  k = kernel(X_train,X_test)
  Cn = kernel(X_train,X_train)
  c = kernel(X_test,X_test)+1

  temp = np.dot(k.transpose(), np.linalg.inv(Cn+  np.eye(len(X_train))))
  mean =np.dot (temp,  T_train)
  cov = c - np.dot(temp,k)

  a = mean.ravel()
  a = a.reshape((len(a),1))

  return a, cov

def mean_cov_gaussian (X_train,T_train,test,type):
  
  X_test = (test)
  k = exp_kernel(X_train,X_test,type)
  Cn = exp_kernel(X_train,X_train,type)
  c = exp_kernel(X_test,X_test,type)+1

  temp = np.dot(k.transpose(), np.linalg.inv(Cn+  np.eye(len(X_train))))
  mean =np.dot (temp,  T_train)
  cov = c - np.dot(temp,k)

  a = mean.ravel()
  a = a.reshape((len(a),1))

  return a, cov

def data_normalize(x):
  x = (x-x.max())/(x.max() - x.min())
  return x

"""## 2.1 Construct a kernel function using the basis functions"""

x = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Homework2/x.csv")
t = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Homework2/t.csv")
x = np.array(x)
t = np.array(t)

# X_train = np.array([3, 1, 4, 5, 9]).reshape(-1, 1)
# T_train = np.array([-0.990102063600402,0.5401609836225097,-0.6536735541146899,0.28352602801580806,-0.91100025015764]).reshape(-1, 1)
# X_test =np.arange(0, 10, 0.2).reshape(-1, 1)
# X_train = data2matrix(X_train)
# x_temp = X_test
# X_test = data2matrix(X_test)

x_temp,t_temp,X_train, T_train, X_test, T_test = data_split(x,t)

k = kernel(X_train,X_test)
Cn = kernel(X_train,X_train)
c = kernel(X_test,X_test)

temp = np.dot(k.transpose(), np.linalg.inv(Cn+ 1 * np.eye(len(X_train))))
temp = np.dot(k.transpose(), np.linalg.inv(Cn))
mean =np.dot (temp,  T_train)
cov = c - np.dot(temp,k)

a = mean.ravel()
a = a.reshape((len(a),1))

error_rms = error_cal(mean,T_train)
print("The root mean square error = ", error_rms)

## create the array of input that has increasing value for drawing prediction curve
test = np.linspace(0,10,150).reshape((-1,1))
mean, cov = mean_cov(X_train,T_train,test)
uncertainty = np.sqrt(np.diagonal(cov)) # fix uncertainty
uncertainty = np.nan_to_num(uncertainty)

plt.plot(x_temp,t_temp,label = "Train",marker='o',linestyle = "None")
plt.plot(test.ravel(),mean.ravel(),label="Predict curve")
plt.legend()
plt.title("Gaussian Process using polynomial kernel")


uper = mean + uncertainty.reshape((-1,1))
lower = mean - uncertainty.reshape((-1,1))
error = error_cal(mean,t_temp)
plt.fill_between(test.flatten(), lower.flatten(), uper.flatten(), alpha=0.3)

plt.show()

"""# 2.2 Exponential-quadratic kernel function
## Data loading
"""

x = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Homework2/x.csv")
t = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Homework2/t.csv")

x_temp,t_temp,X_train, T_train, X_test, T_test = data_split_gaussian(x,t)

# X_train = np.array([3, 1, 4, 5, 9]).reshape(-1, 1)
# T_train = np.array([-0.990102063600402,0.5401609836225097,-0.6536735541146899,0.28352602801580806,-0.91100025015764]).reshape(-1, 1)
# X_test =np.array([3,5,6,2,1]).reshape(-1, 1)

# X_train = data_normalize(X_train)
# T_train = data_normalize(T_train)
# X_test = data_normalize(X_test)


# X_train = data2matrix(X_train)
# x_temp = X_test
# X_test = data2matrix(X_test)

"""## 1. Linear kernel"""

##/ Linear kernel
type = "linear"
k = exp_kernel(X_train,X_test,type)
Cn = exp_kernel(X_train,X_train,type)
c = exp_kernel(X_test,X_test, type)

temp = np.dot(k.transpose(), np.linalg.inv(Cn+ 1e-8 * np.eye(len(X_train))))
mean =np.dot (temp,  T_train)
cov = c - np.dot(temp,k)

a = mean.ravel()
a = a.reshape((len(a),1))

error_rms = error_cal(mean,T_train)
print("The root mean square error = ", error_rms)

## create the array of input that has increasing value for drawing prediction curve
test = np.linspace(0,10,100).reshape((-1,1))
mean, cov = mean_cov_gaussian(X_train,T_train,test,type= type)
uncertainty = np.sqrt(cov.diagonal()) # fix uncertainty
uncertainty = np.nan_to_num(uncertainty)

uper = mean + uncertainty.reshape((-1,1))
lower = mean - uncertainty.reshape((-1,1))
error = error_cal(mean,t_temp)

plt.plot(x_temp,t_temp,label = "Train",marker='o',linestyle = "None")

plt.plot(test.ravel(),mean.ravel(),label="Predict curve")
plt.fill_between(test.ravel(),lower.ravel(),uper.ravel(), alpha=0.2,facecolor = "blue")
plt.title("Gaussian Process using Linear kernel θ = {0, 0, 0, 1}")

plt.legend()
plt.show()

"""

---


## 2. Squared exponential """

##/ Squared exponential kernel
type = "squared"
k = exp_kernel(X_train,X_test,type)
Cn = exp_kernel(X_train,X_train,type)
c = exp_kernel(X_test,X_test, type)

temp = np.dot(k.transpose(), np.linalg.inv(Cn+ 1e-8 * np.eye(len(X_train))))
mean =np.dot (temp,  T_train)
cov = c - np.dot(temp,k)

a = mean.ravel()
a = a.reshape((len(a),1))

error_rms = error_cal(mean,T_train)
print("The root mean square error = ", error_rms)

## create the array of input that has increasing value for drawing prediction curve
test = np.linspace(0,10,100).reshape((-1,1))
mean, cov = mean_cov_gaussian(X_train,T_train,test,type= type)
uncertainty = np.sqrt(cov.diagonal()) # fix uncertainty
uncertainty = np.nan_to_num(uncertainty)

uper = mean + uncertainty.reshape((-1,1))
lower = mean - uncertainty.reshape((-1,1))
error = error_cal(mean,t_temp)

plt.plot(x_temp,t_temp,label = "Train",marker='o',linestyle = "None")

plt.plot(test.ravel(),mean.ravel(),label="Predict curve")
plt.fill_between(test.ravel(),lower.ravel(),uper.ravel(), alpha=0.2,facecolor = "blue")
plt.title("Gaussian Process using Squared exponential kernel θ = {1, 1, 0, 0}")

plt.legend()
plt.show()

"""

---


## 3. Exponential-quadratic kernel (type 1) 



"""

##/ Exponential-quadratic kernel (type 1)
type = "exp1"
k = exp_kernel(X_train,X_test,type)
Cn = exp_kernel(X_train,X_train,type)
c = exp_kernel(X_test,X_test, type)

temp = np.dot(k.transpose(), np.linalg.inv(Cn+ 1e-8 * np.eye(len(X_train))))
mean =np.dot (temp,  T_train)
cov = c - np.dot(temp,k)

a = mean.ravel()
a = a.reshape((len(a),1))

error_rms = error_cal(mean,T_train)
print("The root mean square error = ", error_rms)

## create the array of input that has increasing value for drawing prediction curve
test = np.linspace(0,10,100).reshape((-1,1))
mean, cov = mean_cov_gaussian(X_train,T_train,test,type= type)
uncertainty = np.sqrt(cov.diagonal()) # fix uncertainty
uncertainty = np.nan_to_num(uncertainty)

uper = mean + uncertainty.reshape((-1,1))
lower = mean - uncertainty.reshape((-1,1))
error = error_cal(mean,t_temp)

plt.plot(x_temp,t_temp,label = "Train",marker='o',linestyle = "None")

plt.plot(test.ravel(),mean.ravel(),label="Predict curve")
plt.fill_between(test.ravel(),lower.ravel(),uper.ravel(), alpha=0.2,facecolor = "blue")
plt.title("Gaussian Process using Exponential-quadratic kernel θ = {1, 1, 0, 16}")

plt.legend()
plt.show()

"""

---


## 3. Exponential-quadratic kernel (type 2) 



"""

##/ Exponential-quadratic kernel (type 2)
type = "exp2"
k = exp_kernel(X_train,X_test,type)
Cn = exp_kernel(X_train,X_train,type)
c = exp_kernel(X_test,X_test, type)

temp = np.dot(k.transpose(), np.linalg.inv(Cn+ 1e-8 * np.eye(len(X_train))))
mean =np.dot (temp,  T_train)
cov = c - np.dot(temp,k)

a = mean.ravel()
a = a.reshape((len(a),1))

error_rms = error_cal(mean,T_train)
print("The root mean square error = ", error_rms)

## create the array of input that has increasing value for drawing prediction curve
test = np.linspace(0,10,100).reshape((-1,1))
mean, cov = mean_cov_gaussian(X_train,T_train,test,type= type)
uncertainty = np.sqrt(cov.diagonal()) # fix uncertainty
uncertainty = np.nan_to_num(uncertainty)

uper = mean + uncertainty.reshape((-1,1))
lower = mean - uncertainty.reshape((-1,1))
error = error_cal(mean,t_temp)

plt.plot(x_temp,t_temp,label = "Train",marker='o',linestyle = "None")

plt.plot(test.ravel(),mean.ravel(),label="Predict curve")
plt.fill_between(test.ravel(),lower.ravel(),uper.ravel(), alpha=0.2,facecolor = "blue")
plt.title("Gaussian Process using Exponential-quadratic kernel θ = {1, 1, 16, 0}")

plt.legend()
plt.show()

"""## 2.4
At first, when calculating the kernel, I use the dot product for the 2 matrix and return the result. And it turned out the result of predictive curve is the straight line only. Furthermore I got stuck to use this approach when the two input matrix into the kernel are different dimension. After doing some research and dicussing with classmate, I found out the better approach that is to calculate the single point with single point, after that concate it back to a matrix. This way, the problem of differences in dimensions are solved. 

The Gaussian Process can directly calculate the predictive curve based on the training data without calculating the weight parameters. With the covariance matrix, we can derive out the uncertainty for each point

"""

