{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D40fSA1xAURe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_feature(x, drop_index=-1):\n",
        "    if drop_index != -1:\n",
        "        name_list = x.index.values.tolist() #get name of the index\n",
        "        x = x.drop([name_list[drop_index]],axis=0)\n",
        "    return x\n",
        "\n",
        "def gaussian_func(x,mu,s):\n",
        "    # return np.exp(- np.linalg.norm(mu-x)**2/(2*s*s))\n",
        "    return np.exp(-(mu-x)**2/(s*s))\n",
        "\n",
        "\n",
        "def polinomial(x):\n",
        "    for i in range (len(x)):\n",
        "        x[i,:] = np.power(x[i,:],i+1)\n",
        "    return x\n",
        "\n",
        "def shuffle(x):\n",
        "    x = np.transpose(x)\n",
        "    np.random.shuffle(x)\n",
        "    x = np.transpose(x)\n",
        "    return x\n",
        "def data_dividing(x,test_percent):\n",
        "    test_percent = test_percent\n",
        "    train_percent = 1 - test_percent\n",
        "    \n",
        "    x_test = x[0:len(x)-1,0:round(len(x[1])*test_percent)]\n",
        "    t_test = x[len(x)-1, 0:round(len(x[1])*test_percent)]\n",
        "    t_test = np.transpose(t_test)\n",
        "    # print(\"x_test.shape\",x_test.shape)\n",
        "    x_train = x[0:len(x)-1,round(len(x[1])*test_percent):len(x[1])]\n",
        "    t_train = x[len(x)-1,round(len(x[1])*test_percent):len(x[1])]\n",
        "    t_train = np.transpose(t_train)\n",
        "    # print(\"x_train.shape = \", x_train.shape)\n",
        "    # print(\"t_train.shape = \", t_train.shape)\n",
        "    \n",
        "    return x_test, t_test, x_train, t_train\n",
        "def data_m2(x_, test_percent=0.1,cross= False):\n",
        "    target_index = x_.shape[0]-1\n",
        "    target = x_[target_index,:]\n",
        "    \n",
        "    # print(\"target_index = \",target_index)\n",
        "    temp = np.zeros((11*11,len(t)))\n",
        "    # print(temp.shape)\n",
        "    x_ = np.concatenate((x_,temp),axis=0)\n",
        "    for k in range(len(t)):\n",
        "        x_temp = x_[0:11,k]\n",
        "        for i in range (1,12):\n",
        "            for j in range (1,12):\n",
        "                temp = x_temp[i-1]*x_temp[j-1]\n",
        "                x_[11*i+j-1][k] = temp\n",
        "    \n",
        "    if (cross==True):\n",
        "      return x_ \n",
        "    \n",
        "    test_percent = test_percent\n",
        "    x_test = x_[:,0:round(x_.shape[1]*test_percent)]\n",
        "    t_test = target[0:round(x_.shape[1]*test_percent)]\n",
        "    t_test = np.reshape(t_test,(len(t_test),1))\n",
        "    \n",
        "    x_train = x_[:,round(x_.shape[1]*test_percent):x_.shape[1]]\n",
        "    t_train = target[round(x_.shape[1]*test_percent):x_.shape[1]]\n",
        "    t_train = np.reshape(t_train,(len(t_train),1))\n",
        "    \n",
        "    return x_test, t_test, x_train, t_train\n",
        "\n",
        "   \n",
        "   \n",
        "    \n",
        "    \n",
        "def cross_data_dividing(x,i,element_num):\n",
        "    fold_test = x[:,i*element_num: (i+1)*element_num]\n",
        "    fold_train = np.delete(x,np.s_[i*element_num:(i+1)*element_num],1)\n",
        "    \n",
        "    t_test = fold_test[fold_test.shape[0]-1,:]\n",
        "    t_test = np.reshape(t_test,(len(t_test),1))\n",
        "    x_test = np.delete(fold_test,fold_test.shape[0]-1,0)\n",
        "    \n",
        "    t_train = fold_train[fold_train.shape[0]-1,:]\n",
        "    t_train = np.reshape(t_train,(len(t_train),1))\n",
        "    x_train = np.delete(fold_train,fold_train.shape[0]-1,0)\n",
        "    \n",
        "    return x_test, t_test, x_train, t_train\n",
        "\n",
        "def error_calculating(x,t,w):\n",
        "    error = 0 \n",
        "    \n",
        "    \n",
        "    for i in range(x.shape[1]):\n",
        "        temp = x[:,i]\n",
        "       \n",
        "        temp = np.array(temp)\n",
        "        temp = temp.reshape(-1,1)\n",
        "        predict_ = np.dot(w.transpose(),temp)\n",
        "        \n",
        "        error = error + 1/2*np.power((predict_ - t[i]),2)\n",
        "        # print(t_test[i])\n",
        "        \n",
        "        \n",
        "    \n",
        "    error_rms = np.sqrt(2*error/(x.shape[1]))\n",
        "    return error_rms\n",
        "\n",
        "def part_of_main(x_test, t_test, x_train, t_train,basis_func_type,regularization):\n",
        "    \n",
        "    \n",
        "    if basis_func_type == \"gaussian\":\n",
        "        # x = data.transpose()\n",
        "        mu_train = np.mean(x_train)\n",
        "        s_train  = np.std(x_train) # recheck again\n",
        "        x_train = gaussian_func(x_train,mu_train,s_train)\n",
        "        \n",
        "        mu_test = np.mean(x_test)\n",
        "        s_test  = np.std(x_test) # recheck again\n",
        "        x_test = np.vectorize(gaussian_func)(x_test,mu_test,s_test)\n",
        "        # print(\"x.shape\",x.shape)\n",
        "        # print('gaussian')\n",
        "    \n",
        "    if basis_func_type == \"sigmoid\":\n",
        "        x_train = 1/(1+np.exp(-x_train))\n",
        "        x_test = 1/(1+np.exp(-x_test))\n",
        "        # print('sigmoid')\n",
        "    \n",
        "    \n",
        "    \n",
        "    if regularization == True:\n",
        "        a = np.dot(x_train,x_train.transpose())+np.identity(len(x_train))*100\n",
        "    \n",
        "    else:\n",
        "        a = np.dot(x_train,x_train.transpose())\n",
        "    \n",
        "    a = np.linalg.pinv(a)\n",
        "    \n",
        "    b = np.dot(x_train,t_train)\n",
        "    \n",
        "    w = (np.dot(a,b))\n",
        "    \n",
        "    error_rms_train = error_calculating(x_train,t_train,w)\n",
        "    error_rms_test = error_calculating(x_test,t_test,w)\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "    error_list[drop_index] = error_rms_test\n",
        "    # print(error_rms)################\n",
        "    \n",
        "    return error_list,error_rms_train, error_rms_test\n",
        "\n",
        "\n",
        "    \n",
        "def main_regression(x,regularization = False, shuffle = False, basis_func_type = \"polinomial\",dividing=\"normal\"):\n",
        "    if shuffle == True:\n",
        "        x = np.transpose(x)\n",
        "        np.random.shuffle(x)\n",
        "        x = np.transpose(x)\n",
        "    # x = np.transpose(x)\n",
        "    # print(\"x.shape\", x.shape)\n",
        "\n",
        "    test_percent = 0.1\n",
        "    if dividing == \"normal\":\n",
        "        x_test, t_test, x_train, t_train = data_dividing(x,test_percent)\n",
        "        error_list,error_rms_train,error_rms_test = part_of_main(x_test, t_test, x_train, t_train,basis_func_type,regularization)\n",
        "        return error_list,error_rms_train,error_rms_test\n",
        "    elif dividing == \"M=2\":\n",
        "        x_test, t_test, x_train, t_train = data_m2(x,test_percent)\n",
        "        error_list,error_rms_train,error_rms_test = part_of_main(x_test, t_test, x_train, t_train,basis_func_type,regularization)\n",
        "        return error_list,error_rms_train,error_rms_test\n",
        "    elif dividing == \"cross_validation\":\n",
        "        k = 5\n",
        "        _data = x\n",
        "        x_m2 = data_m2(_data,cross=True)\n",
        "        element_num = round(x.shape[1]/k)\n",
        "        # 0 - 319 = i*element_num -> (i+1)*element_num - 1\n",
        "        # 320 - 639\n",
        "        total_error_rms = 0\n",
        "        for i in range (k):\n",
        "           x_test, t_test, x_train, t_train = cross_data_dividing(x,i,element_num)\n",
        "           error_list,error_rms_train,error_rms_test = part_of_main(x_test, t_test, x_train, t_train,basis_func_type,regularization)\n",
        "           total_error_rms = total_error_rms + error_rms_test\n",
        "        \n",
        "        mean_error_m1 = total_error_rms / k\n",
        "\n",
        "        total_error_rms = 0\n",
        "        basis_func_type = \"sigmoid\"\n",
        "        for i in range (k):\n",
        "           x_test, t_test, x_train, t_train = cross_data_dividing(x,i,element_num)\n",
        "           error_list,error_rms_train,error_rms_test = part_of_main(x_test, t_test, x_train, t_train,basis_func_type= basis_func_type,regularization=False)\n",
        "           total_error_rms = total_error_rms + error_rms_test\n",
        "        \n",
        "        mean_error_sigmoid = total_error_rms / k\n",
        "\n",
        "        total_error_rms = 0\n",
        "        basis_func_type = \"gaussian\"\n",
        "        for i in range (k):\n",
        "           x_test, t_test, x_train, t_train = cross_data_dividing(x,i,element_num)\n",
        "           error_list,error_rms_train,error_rms_test = part_of_main(x_test, t_test, x_train, t_train,basis_func_type= basis_func_type,regularization=False)\n",
        "           total_error_rms = total_error_rms + error_rms_test\n",
        "        \n",
        "        mean_error_gaussian= total_error_rms / k\n",
        "        # print (\"mean_error = \", mean_error_m1) \n",
        "        return mean_error_m1, mean_error_sigmoid, mean_error_gaussian\n",
        "        \n",
        "    \n",
        "\n",
        "   \n",
        "    "
      ],
      "metadata": {
        "id": "kPGj_2UDoihP"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Homework1/X.csv')\n",
        "t = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Homework1/T.csv')\n",
        "\n",
        "data = pd.concat([data,t],axis=1)\n",
        "D = 11\n"
      ],
      "metadata": {
        "id": "-nRUzjgDoq6q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CISkSzwPo0a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1 Feature Selection\n",
        "**2.1 a**\n",
        "\n",
        "\n",
        "> The code below shows the results of Error_rms for training and testing in case M = 1 and M = 2. We can see that, in most of the cases, the higher order, M=2, would give us smaller errors compared to the errors of M=1. When we apply the pre-trained weight to the train data set, we usually get the smaller error_rms because these data has been seen by the model before. On the contrast, the data of testing has not been seen by the model, which leads to the results of higher error_rms.\n",
        "\n"
      ],
      "metadata": {
        "id": "DDix3c_M5tLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "#For M=1 \n",
        "data_ =data.sample(frac=1)\n",
        "temp = data_\n",
        "x_ = temp.transpose()\n",
        "x_ = np.array(x_)\n",
        "temp, error_rms_train, error_rms_test = main_regression(x_)\n",
        "print (\"Error_rms_train, error_rms_test for M = 1 are = {}, {}\".format(error_rms_train, error_rms_test))\n",
        "\n",
        "#For M=2\n",
        "temp = data_\n",
        "x_ = temp.transpose()\n",
        "x_ = np.array(x_)\n",
        "temp, error_rms_train, error_rms_test = main_regression(x_,dividing=\"M=2\")\n",
        "print (\"Error_rms_train, error_rms_test for M = 2 are = {}, {}\".format(error_rms_train, error_rms_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efuYLp5sqglp",
        "outputId": "00a4a4f3-a52d-4c61-daf9-e8aa89df4429"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error_rms_train, error_rms_test for M = 1 are = [0.65759591], [0.53138046]\n",
            "Error_rms_train, error_rms_test for M = 2 are = [[0.61748178]], [[0.56016763]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**2.1 b**\n",
        "\n",
        "\n",
        "> I would try to drop each feature of the input data and calculating the Error_rms then save them in the list. After that, I would plot all of the error in order to compare of these, the one that leads to the largest error would be the most important Fearture of the input data. As we can see in the figure, the error is largest when we drop the index 10, which mean the **Alcohol feature**. \n",
        "\n"
      ],
      "metadata": {
        "id": "4fc1mWVP8e1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For M=1\n",
        "feature_test = True\n",
        "error_list = np.zeros((D,1))\n",
        "if feature_test == True:\n",
        "    temp_data = data.sample(frac=1)\n",
        "    t = temp_data.iloc[:,11]\n",
        "    t = np.array(t)\n",
        "    t = np.reshape(t, (len(t),-1))\n",
        "    # temp_data = temp_data.drop('quality',axis=1) #drop the index column \n",
        "    for drop_index in range (11):\n",
        "        _data = temp_data.transpose()\n",
        "        _data = drop_feature(_data,drop_index) #drop each feature to observe which one contributes the most\n",
        "        \n",
        "        _data = np.array(_data)\n",
        "        error_list, temp, temp1 = main_regression(_data)\n",
        "        print(\"The Error_rms of testing data when droping the {} feature is = {}\".format(drop_index,temp1))\n",
        "    plt.plot(error_list) #2.1b\n",
        "    plt.xlabel(\"Features\")\n",
        "    plt.ylabel(\"Error\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "JXzzHhCl8jS1",
        "outputId": "387de38b-c284-4b08-9c34-f817b42e0160"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Error_rms of testing data when droping the 0 feature is = [0.59752384]\n",
            "The Error_rms of testing data when droping the 1 feature is = [0.61363439]\n",
            "The Error_rms of testing data when droping the 2 feature is = [0.59865053]\n",
            "The Error_rms of testing data when droping the 3 feature is = [0.595349]\n",
            "The Error_rms of testing data when droping the 4 feature is = [0.59504975]\n",
            "The Error_rms of testing data when droping the 5 feature is = [0.59860518]\n",
            "The Error_rms of testing data when droping the 6 feature is = [0.60452992]\n",
            "The Error_rms of testing data when droping the 7 feature is = [0.60962873]\n",
            "The Error_rms of testing data when droping the 8 feature is = [0.59868631]\n",
            "The Error_rms of testing data when droping the 9 feature is = [0.60635306]\n",
            "The Error_rms of testing data when droping the 10 feature is = [0.65819022]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.2 Maximum likelihood approach**\n",
        "2.2 ab) The code below will try different basis functions and comparing the result of Error_rms. As we can see from the result, it turns out the error_rms would be the smallest one when the basis function is Polynomial. Since the amount of data that we have is not really complicated, therefore, we can choose the polynomial function to deal with this kind of problem. Since I remember from what professor said in the class. If the problem is easy, the complex model may not give the best result. And by contrast, the complex problem can't be solved by unsophisticated model. As the result, we should test different type of model to find out the best one."
      ],
      "metadata": {
        "id": "TKl_CCpr-3kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "basis_func_test = True\n",
        "basis_function = [\"none\",\"polinomial\",\"gaussian\",\"sigmoid\"]\n",
        "basis_func_type = basis_function[2]\n",
        "if basis_func_test == True:\n",
        "    x = data.sample(frac=1)\n",
        "    x = x.transpose()\n",
        "    x = np.array(x)\n",
        "    for i in range (1,4):\n",
        "        basis_func_type = basis_function[i]\n",
        "        temp, error_rms_train, error_rms_test = main_regression(x,basis_func_type=basis_func_type)\n",
        "        print(\"Error_rms of testing data when using '{}' basis function is {}\".format(basis_function[i],error_rms_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvM-DOmf_Khx",
        "outputId": "d19a32be-4294-4276-f290-b12421777bd6"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error_rms of testing data when using 'polinomial' basis function is [0.67752733]\n",
            "Error_rms of testing data when using 'gaussian' basis function is [0.6891307]\n",
            "Error_rms of testing data when using 'sigmoid' basis function is [0.72934853]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**2.2 c**\n",
        "\n",
        "\n",
        "> We will dividing the data into 5 bins for this method and apply this method to compare between the two, \"polynomial\" \"gaussian\" and \"sigmoid\" basis function. As we can see, the polynomial basis function again gives the best result. \n",
        "\n"
      ],
      "metadata": {
        "id": "HKTFoYSdDkQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Nfold Cross validation\n",
        "x =data.sample(frac=1)\n",
        "x = x.transpose()\n",
        "x = np.array(x)\n",
        "mean_error_rms_m1, mean_error_rms_sigmoid, mean_error_rms_gaussian= main_regression(x,dividing=\"cross_validation\")\n",
        "print(\"mean_error_rms_m1 = \" ,mean_error_rms_m1)\n",
        "print(\"mean_error_rms_gaussian = \", mean_error_rms_gaussian)\n",
        "print(\"mean_error_rms_sigmoid = \", mean_error_rms_sigmoid)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnxsCSjPrbC3",
        "outputId": "667ae65e-3799-48f1-f44c-9b839f56845b"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_error_rms_m1 =  [[0.65336991]]\n",
            "mean_error_rms_gaussian =  [[0.66739306]]\n",
            "mean_error_rms_sigmoid =  [[0.69044895]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 2.3 Maximum a posteriori approach\n",
        "\n",
        "\n",
        "\n",
        "> 2.3a) The key difference is that MAP uses both likelihood and prior while Maximum likelihood doesn't use prior.\n",
        "\n",
        "\n",
        ">2.3 b c) In the comparison, we see that the Error_rms with regularization gives 0.8210171, meanwhile, the error_rms without regularization is 0.67081066. As the consequence, the results show that the model with regularization does not necessarily sends out the better results.\n",
        "\n"
      ],
      "metadata": {
        "id": "BdvNC47gM27b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_ =data.sample(frac=1)\n",
        "temp = data_\n",
        "x_ = temp.transpose()\n",
        "x_ = np.array(x_)\n",
        "temp, error_rms_train, error_rms_regularization = main_regression(x_,basis_func_type = \"gaussian\",regularization=True)\n",
        "temp, error_rms_train, error_rms_without_reg = main_regression(x_,basis_func_type = \"gaussian\",regularization=False)\n",
        "print(\"Error_rms with regularization is {}\".format( error_rms_regularization))\n",
        "print(\"Error_rms with wihtout regularization is {}\".format(error_rms_without_reg))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTo61Ig5NAsT",
        "outputId": "11a86c3d-d8a9-4020-a6de-4a9f8afb4d55"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error_rms with regularization is [0.86026683]\n",
            "Error_rms with wihtout regularization is [0.66717473]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "t-qZjoFEWb2c"
      }
    }
  ]
}